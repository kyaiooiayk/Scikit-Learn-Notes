{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Introduction</a></span></li><li><span><a href=\"#Imports\" data-toc-modified-id=\"Imports-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Imports</a></span></li><li><span><a href=\"#Create-a-CV-score-harness\" data-toc-modified-id=\"Create-a-CV-score-harness-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Create a CV score harness</a></span></li><li><span><a href=\"#Averaging-the-models\" data-toc-modified-id=\"Averaging-the-models-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Averaging the models</a></span></li><li><span><a href=\"#Blending-the-models\" data-toc-modified-id=\"Blending-the-models-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Blending the models</a></span></li><li><span><a href=\"#References\" data-toc-modified-id=\"References-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>References</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "<hr style=\"border:2px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<font color=black>\n",
    "\n",
    "**What?** How to create a class with fit and predict methods\n",
    "\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports\n",
    "<hr style=\"border:2px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold, cross_val_score, cross_validate\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a CV score harness\n",
    "<hr style=\"border:2px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<font color=black>\n",
    "\n",
    "- **k-Fold CV** is the gold standard for evaluating the performance of a machine learning algorithm on unseen data with k set to 3, 5, or 10. CV is an approach that you can use to estimate the performance of a ML algorithm with less variance than a single train-test set split. It works by splitting the dataset into k-parts. After running cross validation you end up with k different performance scores that you can summarize using a mean and a standard deviation.\n",
    "\n",
    "- **Train/test split** is good for speed when using a slow algorithm and produces performance estimates with lower bias when using large datasets. We can take our original dataset and split it into two parts. Train the algorithm on the first part, make predictions on the second part and evaluate the predictions against the expected results. CONS: A downside of this technique is that it can have a high variance. This means that differences in the training and test dataset can result in meaningful differences in the estimate of accuracy.\n",
    "\n",
    "- **leave-one-out**: You can configure cross validation so that the size of the fold is 1 (k is set to the number of observations in your dataset). This variation of cross validation is called leave-one-out cross validation. The result is a large number of performance measures that can be summarized in an effort to give a more reasonable estimate of the accuracy of your model on unseen data. CONS: A downside is that it can be a computationally more expensive procedure than k-fold cross validation. You can see in the standard deviation that the score has MORE variance than the k-fold cross validation results described above\n",
    "\n",
    "- **repeated random splits** Another variation on k-fold cross validation is to create a random split of the data like the train/test split described above, but repeat the process of splitting and evaluation of the algorithm multiple times, like cross validation. This has the speed of using a train/test split and the reduction in variance in the estimated performance of k-fold cross validation. CONS: A down side is that repetitions may include much of the same data in the train or the test split from run to run, introducing redundancy into the evaluatio\n",
    "\n",
    "- **GroupKFold** GroupKFold is a variation of k-fold which ensures that the same group is not represented in both testing and training sets. For example if the data is obtained from different subjects with several samples per-subject and if the model is flexible enough to learn from highly person specific features it could fail to generalize to new subjects. GroupKFold makes it possible to detect this kind of overfitting situations.\n",
    "\n",
    "- **The bottom line?** If in doubt, use 10-Fold CV.\n",
    "  \n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<font color=black>\n",
    "\n",
    "| Methods                           | Type/variation of | Speed                        | Variance                   |\n",
    "| --------------------------------- | ----------------- | ---------------------------- | -------------------------- |\n",
    "| Train/test split                  | bare minimum      | Fastest                      | higher than k-fold         |\n",
    "| k-Fold                            | k-Fold            | Slower than train/test split | less than train/test split |\n",
    "| Leave-one-out                     | k-Fold            | Slower tha k-Fold            | higher than k-Fold         |\n",
    "| Repeated Random Test-Train Splits | Train/split       | Slower than train/test split | less than k-Fold           |\n",
    "| Group k-Fold                      | k-Fold            | ?                            | ?                          |\n",
    "\n",
    "\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def get_cv_scores(model, train_set, train_target, name, n_splits=10, state=42, test_size=0.25,\n",
    "                  type_=\"k-fold\", verbose=False):\n",
    "    \"\"\"Get the CV scores of the model.\n",
    "\n",
    "    To see the available keys\n",
    "    sklearn.metrics.SCORERS.keys()\n",
    "\n",
    "    The loocv does not have R2 because the function returns aNaN. \n",
    "    This could be fixed but I have not looked into it.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : object\n",
    "\n",
    "    train_set : pandas dataframe\n",
    "        Matrix contraining instances and features\n",
    "\n",
    "    train_target : pandas dataframe\n",
    "        target\n",
    "\n",
    "    name : string\n",
    "        name to be used in returns pandas dataframe\n",
    "\n",
    "    n_splits : int, default=10\n",
    "        No of splits\n",
    "\n",
    "    state : int, default=42\n",
    "        No for the random state pseudo number generator\n",
    "\n",
    "    test_size : float\n",
    "        Size of the test set between 0 and 1\n",
    "\n",
    "    type_ : string, default=\"k-fold\"\n",
    "        Type of cross-validation used\n",
    "\n",
    "    verbose : string, default=False\n",
    "        If True print on screen the results, otherwise it does \n",
    "        not print anything on console.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    table : pandas dataframe\n",
    "        Table containing the mean and std for each metrics.\n",
    "\n",
    "    split_strategy : iterator\n",
    "        Iterator used in the call, which can be used to access each fold or\n",
    "        training set splits.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Type of CV selected\", type_)\n",
    "\n",
    "    # k-Fold\n",
    "    if type_ == \"k-fold\":\n",
    "        scoring = [\"neg_mean_absolute_error\", \"neg_mean_squared_error\",\n",
    "                   \"neg_root_mean_squared_error\", \"r2\"]\n",
    "        metrics_acronyms = [\"MAE\", \"MSE\", \"RMSE\", \"R2\"]\n",
    "        split_strategy = KFold(\n",
    "            n_splits=n_splits, random_state=state, shuffle=True)\n",
    "        result = cross_validate(model, train_set, train_target,\n",
    "                                scoring=scoring, cv=split_strategy, n_jobs=-1, return_train_score=True)\n",
    "    # Repeated train-test\n",
    "    elif type_ == \"repeated_tt\":\n",
    "        scoring = [\"neg_mean_absolute_error\", \"neg_mean_squared_error\",\n",
    "                   \"neg_root_mean_squared_error\", \"r2\"]\n",
    "        metrics_acronyms = [\"MAE\", \"MSE\", \"RMSE\", \"R2\"]\n",
    "        split_strategy = ShuffleSplit(\n",
    "            n_splits=n_splits, test_size=test_size, random_state=state)\n",
    "        result = cross_validate(model, train_set, train_target,\n",
    "                                scoring=scoring, cv=split_strategy, n_jobs=-1, return_train_score=True)\n",
    "\n",
    "    elif type_ == \"loocv\":\n",
    "        scoring = [\"neg_mean_absolute_error\", \"neg_mean_squared_error\",\n",
    "                   \"neg_root_mean_squared_error\"]\n",
    "        metrics_acronyms = [\"MAE\", \"MSE\", \"RMSE\"]\n",
    "        split_strategy = LeaveOneOut()\n",
    "        result = cross_validate(model, train_set, train_target,\n",
    "                                scoring=scoring, cv=split_strategy, n_jobs=-1, return_train_score=True)\n",
    "\n",
    "    mean, std = [], []\n",
    "    for i in scoring:\n",
    "        if verbose == True:\n",
    "            print(\"***********\")\n",
    "            print(\"Scoring: \", i)\n",
    "            print(\"Folds:\", [abs(i) for i in result[\"test_\"+i]])\n",
    "            print(\"Mean: %.6f\" % abs(result[\"test_\"+i].mean()))\n",
    "            print(\"Standard deviation: %.6f\" % abs(result[\"test_\"+i].std()))\n",
    "        mean.append(abs(result[\"test_\"+i].mean()))\n",
    "        std.append(abs(result[\"test_\"+i].std()))\n",
    "\n",
    "    table = pd.DataFrame()\n",
    "    table[\"metrics\"] = metrics_acronyms\n",
    "    table[\"mean_CV_\"+type_+\"_\"+name] = mean\n",
    "    table[\"std_CV_\"+type_+\"_\"+name] = std\n",
    "\n",
    "    return table, split_strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Averaging the models\n",
    "<hr style=\"border:2px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin):\n",
    "    def __init__(self, models):\n",
    "        self.models = models\n",
    "\n",
    "    # we define clones of the original models to fit the data in\n",
    "    def fit(self, X, y):\n",
    "        self.models_ = [clone(x) for x in self.models]\n",
    "\n",
    "        # Train cloned base models\n",
    "        for model in self.models_:\n",
    "            model.fit(X, y)\n",
    "\n",
    "        return self\n",
    "\n",
    "    # Now we do the predictions for cloned models and average them\n",
    "    def predict(self, X):\n",
    "        predictions = np.column_stack([\n",
    "            model.predict(X) for model in self.models_\n",
    "        ])\n",
    "        return np.mean(predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average = AveragingModels(models = (model_1, model_2, model_3, model_4))\n",
    "\n",
    "cv_scores_average, _ = get_cv_scores(\n",
    "    average, X, y, name=\"average\", n_splits=10, state=42, test_size=0.25, type_=\"k-fold\", verbose=False)\n",
    "cv_scores_average"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blending the models\n",
    "<hr style=\"border:2px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<font color=black>\n",
    "\n",
    "- The blending is different from the average in that the weights can be specified. \n",
    "- If the weightes are all equal and if they all sum to 1 then this is equivalent to using `Averaging Models`.\n",
    "\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlendingModels(BaseEstimator, RegressorMixin, TransformerMixin):\n",
    "    def __init__(self, models, weights):\n",
    "        self.models = models\n",
    "        self.weights = weights\n",
    "\n",
    "    # we define clones of the original models to fit the data in\n",
    "    def fit(self, X, y):\n",
    "        self.models_ = [clone(x) for x in self.models]\n",
    "\n",
    "        # Train cloned base models\n",
    "        for model in self.models_:\n",
    "            model.fit(X, y)\n",
    "        return self\n",
    "\n",
    "    # Now we do the predictions for cloned models and average them\n",
    "    def predict(self, X):\n",
    "        # \"\"\"\n",
    "        predictions = np.column_stack([\n",
    "            model.predict(X) for model in self.models_\n",
    "        ])\n",
    "        # return np.mean(predictions, axis=1)\n",
    "        # return np.average(predictions, axis=1, weights = self.weights)\n",
    "        return np.average(predictions, axis=1, weights=self.weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blend = BlendingModels(\n",
    "    models=(model_1, model_2, model_3, model_4),\n",
    "weights = [0.25, 0.25, 0.25, 0.25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_scores_blend, _ = get_cv_scores(\n",
    "    blend, X, y, name=\"blend\", n_splits=10, state=42, test_size=0.25, type_=\"k-fold\", verbose=False)\n",
    "cv_scores_blend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "<hr style=\"border:2px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<font color=black>\n",
    "\n",
    "- https://www.kaggle.com/serigne/stacked-regressions-top-4-on-leaderboard#Modelling\n",
    "- [`numpy.average`](https://numpy.org/doc/stable/reference/generated/numpy.average.html)\n",
    "\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "trainingAI",
   "language": "python",
   "name": "trainingai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
